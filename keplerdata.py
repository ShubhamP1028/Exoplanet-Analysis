# -*- coding: utf-8 -*-
"""keplerData.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/191tkYrr3fzBdvXIHnd9K4fzPqvyeenU-
"""

#!/usr/bin/env python3

"""Data Cleaning and Preprocessing Script for Kepler Exoplanet Dataset

This script reads in the 'KeplerExoRaw.csv' file, performs initial cleaning,
and saves a cleaned version. The cleaning workflow includes:
  - Reading the CSV file into a Pandas DataFrame
  - Inspecting the data (head, summary statistics)
  - Renaming columns (if needed) and converting data types
  - Handling missing values (dropping or imputing)
  - Removing duplicate records
  - Saving the cleaned dataset for later analysis

"""

import numpy as np
import pandas as pd

df= pd.read_csv('KeplerExoRaw.csv')

df.head()

def inspect_data(df):
    """Print basic information about the DataFrame."""
    print("----- Dataset Head -----")
    print(df.head())
    print("\n----- Dataset Info -----")
    print(df.info())
    print("\n----- Summary Statistics -----")
    print(df.describe(include='all'))

def clean_column_names(df):
    """    Clean column names: trim extra spaces, convert to lower case, and replace spaces with underscores."""
    df.columns = [col.strip().lower().replace(' ', '_') for col in df.columns]
    return df

def handle_missing_values(df, threshold=0.5):
    """Handle missing values based on a threshold.
    Drop columns with more than threshold proportion of missing values,
    and impute or drop rows for the rest."""
    # Drop columns where missing values exceed a threshold
    missing_ratio = df.isnull().mean()
    cols_to_drop = missing_ratio[missing_ratio > threshold].index
    if len(cols_to_drop) > 0:
        print(f"Dropping columns with > {threshold*100:.0f}% missing values: {list(cols_to_drop)}")
        df = df.drop(columns=cols_to_drop)

    # For the remaining columns, fill numeric columns with median and categorical columns with mode
    for col in df.columns:
        if df[col].dtype in [np.float64, np.int64]:
            median = df[col].median()
            df[col] = df[col].fillna(median)
        else:
            mode = df[col].mode().iloc[0] if not df[col].mode().empty else "Unknown"
            df[col] = df[col].fillna(mode)

    # Optionally: drop any remaining rows with missing values
    # df = df.dropna()
    return df

def remove_duplicates(df):
    """Remove duplicate records from the DataFrame."""
    initial_count = len(df)
    df = df.drop_duplicates()
    final_count = len(df)
    print(f"Removed {initial_count - final_count} duplicate rows.")
    return df

def convert_data_types(df):
    """
    Convert data types if necessary.
    Example: Convert orbit period to numeric if it's not.
    Adjust the columns based on actual dataset schema.
    """
    # Example: Convert 'orbital_period' to numeric (if exists)
    if 'orbital_period' in df.columns:
        df['orbital_period'] = pd.to_numeric(df['orbital_period'], errors='coerce')

    # Example: Convert 'planet_radius' to numeric (if exists)
    if 'planet_radius' in df.columns:
        df['planet_radius'] = pd.to_numeric(df['planet_radius'], errors='coerce')

    # Add any other type conversions based on your exploration
    return df

def save_clean_data(df, output_path):
    """Save the cleaned DataFrame to a CSV file."""
    try:
        df.to_csv(output_path, index=False)
        print(f"Cleaned data saved to {output_path}")
    except Exception as e:
        print(f"Error saving clean data: {e}")
        raise

df = clean_column_names(df)

df = handle_missing_values(df, threshold=0.5)

df = remove_duplicates(df)

df = convert_data_types(df)

inspect_data(df)

df.head()

df.shape

save_clean_data(df, 'cleankepler.csv')

